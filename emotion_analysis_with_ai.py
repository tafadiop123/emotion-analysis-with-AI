# -*- coding: utf-8 -*-
"""Emotion Analysis with AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ijZTJJSeuePOhjYu8Y1Q5WsMIyehPwnR

*   Le but du projet est de faire la classification des émotions des personnes en se basant sur leurs images.
*   Dans cet étude de cas on va considéré qu'on est consultant en AI/ML et qu'on a été recruté par une start-up à San Diego afin de construire, d'entrainer et de déployer un système qui monitore automatiquement les émotions et les expressions faciales des personnes.
*   L'équipe a collecté plus de 20 milles images de faces humaines, avec leurs expressions faciales labélisées et environ 2000 images avec leurs annotations des points-clés faciales.

# Partie 1 : Détection des points faciales clés  

* Dans la première partie, on va créer un modèle de deep learning basé sur les Réseaux de Neurones Convolutionnels et des Blocks Résiduels pour calculer ( ou prédire ) les points faciales clés.
* Le dataset qu'on a ici comporte des coordonnées (x et y) de 15 points faciales clés.
* Les images d'input sont au format 96X96 pixels.
* Les images sont seulement à la couleur Noire et Blanche.

## 1 - Importation des Librairies et des Datasets
"""

# Mount the drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Colab Notebooks/Modern Artificial Intelligence Masterclass UDEMY/

# Import the necessary packages

import pandas as pd
import numpy as np
import os
import PIL
import seaborn as sns
import pickle
from PIL import *
import cv2
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
from IPython.display import display
from tensorflow.python.keras import *
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, optimizers
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import *
from tensorflow.keras import backend as K
from keras import optimizers
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from google.colab.patches import cv2_imshow

dataset_Dr = "/content/drive/My Drive/Colab Notebooks/Modern Artificial Intelligence Masterclass UDEMY/Emotion AI Dataset/"

# On charge le dataset des points faciales clés
keyfacial_df = pd.read_csv(dataset_Dr+"data.csv")

keyfacial_df

#Obtenir des informations relevantes du dataset
keyfacial_df.info()

# Vérifier si le dataframe contient des valeurs nulles 
keyfacial_df.isnull().sum()

# Vérifier le nombre d'images contenu dans le dataframe
keyfacial_df["Image"].shape

#Ici on va appliquer une fonction Lambda de ( x ) qui  va pendre n'importe quel élément x présent 
# dans la colonne "Image" et va le convertir en un array Numpy en utilisant la fonction "np.fromstring" et
#  enfin va convertir l'array 1D en array 2D avec des dimensions (96 x 96)
keyfacial_df["Image"] = keyfacial_df["Image"].apply(lambda  x: np.fromstring(x, dtype=int, sep=' ').reshape(96,96))

# Obtenir la dimension de la première image
keyfacial_df["Image"][0].shape

keyfacial_df['right_eye_center_x'].min()

keyfacial_df['right_eye_center_x'].max()

#Les statistiques descriptives de mon dataframe
keyfacial_df.describe()

"""## 2 : Visualisation des images """

"""
#  Dans cette partie on va visualiser une image aléatoirement dans le Dataframe 
 - Les images sont obtenues à partir du datframe sont représentées en utilisant la foncton "plt.imshow" 
 et son accompagnés de 15 coordonnées ( x et y )
 - Lorsque les coordonnées x sont dans les colonnes dont les indexes sont (0,2,4,...), alors les coordonnées y
 ont des indexes impaires (1,3,5,...) et pour accéder à leurs valeurs on utilise la fonction ".loc"
  
"""
i = np.random.randint(1, len(keyfacial_df))
plt.imshow(keyfacial_df["Image"][i], cmap= "gray")
# Maintenat on représente les coordonnées des points faciales clés
for j in range(1, 31, 2):
  plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], "rx")
  # L'argument "rx" signifie (Red X), c'est la couleur et la forme d'une coordonnée

# Maintenant on va faire la représentation de plusieurs images dans un Grid de 4 lignes et colonnes
fig = plt.figure(figsize=(20,20))

for i in range(16):
  ax = fig.add_subplot(4, 4, i+1)
  image = plt.imshow(keyfacial_df["Image"][i], cmap="gray")
  for j in range(1, 31, 2):
    plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], "rx")

import random
# Challenge : Visualisation aléatoire de 64 nouvelles images dans une grille 
fig = plt.figure(figsize=(20,20))

for i in range(64):
  k = random.randint(1, len(keyfacial_df))
  ax = fig.add_subplot(8, 8, i+1)
  image = plt.imshow(keyfacial_df["Image"][k], cmap="gray")
  for j in range(1, 31, 2):
    plt.plot(keyfacial_df.loc[k][j-1], keyfacial_df.loc[k][j], "rx")

"""## 3 : Augmentation d'image

*   L'augmentation d'image consiste à fabriquer un dataset additionnel en appliquant sur les images des rotations, augmenter la luminosité, etc.. Et en faisant cela on peut augmenter les capacités de généralisation du modèle.
*   Élément de liste



"""

#On crée une copie du dataframe
import copy
keyfacial_df_copy = copy.copy(keyfacial_df)

#On récupère les colonnes du dataframe sans la colonne "Image" d'où le [:-1]
columns = keyfacial_df_copy.columns[:-1]
columns

# Inversion Horizontale - On va faire une inversion sur l'axe y
keyfacial_df_copy["Image"] = keyfacial_df_copy["Image"].apply(lambda x: np.flip(x, axis = 1))

#Lorsqu'on fait l'inversion horizontale, les valeurs de coordonnées de y devrait être pareilles
#Seule les valeurs de coordonnées de x changent.
for i in range(len(columns)):
  if i%2 == 0:
    #Cette fonction permet de faire la rotation
    keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x))

#Représenter l'image Originale
plt.imshow(keyfacial_df["Image"][0], cmap= "gray")
for j in range(1,31,2):
  plt.plot(keyfacial_df.loc[0][j-1], keyfacial_df.loc[0][j], "rx")

#Représenter l'image inversée
plt.imshow(keyfacial_df_copy["Image"][0], cmap= "gray")
for j in range(1,31,2):
  plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], "rx")

# Concatener le dataframe original au Dataframe augmenté
augmented_df = np.concatenate((keyfacial_df, keyfacial_df_copy))
augmented_df.shape

"""
Ensuite on va augmenter la brillance des images. Pour cela on va multiplier les valeurs de pixel par
des valeurs aléatoires comprises entre 1,5 et 2 afin d'augmenter la brillance des images. 
Il faut noter que pour éviter les valeurs de pixel dépassent 255, on va limiter la valeur de la multiplication 
en se servant de la fonction "np.clip()"
"""

import random

keyfacial_df_copy = copy.copy(keyfacial_df)
keyfacial_df_copy["Image"] =  keyfacial_df_copy["Image"].apply(lambda x : np.clip(random.uniform(1.5, 2) * x, 0.0, 255.0))
augmented_df = np.concatenate((augmented_df, keyfacial_df_copy))
augmented_df.shape

# Afficher l'image avec la brillance augmentée

plt.imshow(keyfacial_df_copy["Image"][0], cmap = "gray")
for j in range(1, 31, 2):
  plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], "rx")

""" ## 5 - Normalisation des données et préparation des données d'entrainement

* En normalisant les données, on améne les valeurs de pixel comprises ente 0 et 255, à une valeure comprise entre 0 et 1. 


"""

# Obtenir la valeur des images qui sont présentent dans la 31ième colonne
img = augmented_df[:,30]

# Normaliser les images 
img = img/255.

# Créer un array vide de dimensions (x, 96, 96, 1) pour alimenter le modèle
X = np.empty((len(img), 96, 96, 1))

# Itérer à travers la liste d'images et ajouter les valeurs d'images à un array vide puis en élargissant 
# ses dimensions de (96, 96) à (96, 96, 1). Cela pour mettre le format du array en format batch, 
# qui est un formaat utilisé pour l'entrainement du modèle 
for i in range(len(img)):
  X[i,] = np.expand_dims(img[i], axis=2)

# Convertir le type de l'array en float3
X = np.asarray(X).astype(np.float32)
X.shape

# Obtenir la valeur des coordonnées x et y qui est sont utilisés comme variable cible
 # Pour cela on récupére toutes les lignes du dataframe "augmented_df" et toutes les colonnes sauf la 
 # 31ième colonne. 
 y = augmented_df[:, :30]
 y = np.asarray(y).astype(np.float32)
 y.shape

# Splitter les données en donnée de train et test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

X_train.shape

X_test.shape

"""## 6 : Comprendre la théorie et l'intuition derrière les réseaux de neurones

### - Modéle mathématique d'une neurone

* Le cerveau contient plus de 100 milliards de neurones communiquantes à travers des signaux electriques et chimiques. Les neurones communiquent entre elles et nous aide à voire, penser et générer des idées. 

* Le cerveau humain apprend en créant des connections au sein des neurones. Les réseaux de neurones artificielles (ANN) sont des modèles de traitement d'informations  inspirés du cerveu humain.

* Les neurones collectent les signaux à partir des chaines d'entrée appelées Dendrites, qui traitent les informations dans son Nucleus et ensuite générent un résultat le long d'une branche mince appelé Axon. 


---


### - Les Fonctions d'activation

* SIGMOID : 
      *   Cette fonction prend les valeurs et les fixe à une valeur comprise entre 0 et 1.  
      *   Cette fonction convertir les nombres négatifs grands en 0 et les nombres positifs en 1
      *   Elle est généralement utilisé dans le layer Output pour le réseau perceptron multichouche


* ReLU (Retified Linear Units)
      *    Si la valeur en input est inférieur à 0 alors cette fonction régle le résultat à 0. C'est fonction permet d'éviter la non linéarité dans le réseau de neurones.
      *    ReLu ne sature pas alors elle évite le problème de la disparition du gradient 
      *    Elle utiise un seuillage simple donc elle efficiente sur le plan du traitement informatique
      *    La fonction est généralement utilisée dan les couches cachées  


* Hyperbolic Tangent
      *    La fonction "Tanh" est similaire fonction Sigmoid mais ça différence est qu'elle transforme es valeurs d'entrées en une valeur comprise entre (-1 et 1), donc les outputs sont centrés sur le zero.
      *    La "Tanh" souffre du problème de la disparition du gradient car elle tue le gradient quand elle saturée
      *    En pratique, la "Tanh" est préférable à la Sigmoïd

## 7 -    Comprendre le processus d'entrainement des réseaux de neurones et l'algorithme "Gradient Descent"



* Une fois que les inputs rentrent dans le réseau de neurones un résultat ou prédiction (Y chapeau) est généré par le modèle. 
* Puis ce résultat est différencié par le vrai résultat (Y) qui est sous forme de label. Le résultat de cette différence constitue l'erreur. 
* Ensuite on recommencement l'entrainement puis on met à jour les poids des neurones.

* Une fois que l'entrainement est fini, on passe maintenant à la phase test, et pour cela on gélé les poids du réseau de neurones, puis on évalue le modéle déjà entrainé sur de nouvelles données de test et ainsi généré de nouvelles prédictions.

### - Diviser les données en jeu de données d'entrainement et de test

*  Le jeu de données est généralement divisé en 80 % pour le training et 20 % pour le testing

*  Parfois, on devra inclure le dataset de la crosse validation également et alors on divise le dataset en segment de 60% , 20% , 20% (les nombres peuvent varier)



      1.   Les données d'entrainement sont utilisés pour le calcul du gradient et la mise à jour des poids. 
      2.   Les données de validations:

            * Sont utilisées pour la cross-validation pour évaluer la qualité de la formation au fur et à mesure de son déroulement
            *  La cross-validation est implémentée pour surmonter l'over-fitting qui apparaît quand l'algorithme se focalise sur les détails de l'ensemble d'entrainement au coût de perdre l'abilité pour la généralisation.
      3.   Les données de test : sont utilisés pour tester le réseau de neurones déja entrainé


### - Gradient Descent 


*  Le "Gradient Descent" est un algorithme d'optimisation utilisé pour obtenir les poids du réseau optimisé et la valeur du biais.

* Il marche en essayant itérativement de minimiser la fonction coût ("cost function"). 

*  Il marche en calculant le gradient de la fonction coût ("cost function") et en se déplaçant dans une direction négative jusqu'à ce que le minimum local ou global soit atteint.

*  Si le positif du gradient est pris, le maximum local ou global est atteint.

*  La taille des étapes (step size) prises sont appelées le "learning rate" 

*  Si le "learning rate" augmente, la surface couverte dans l'espace de recherche va augmenter aussi, alors on pourrait atteindre rapidement le minimum global 

*  Mais, on peut dépasser la cible. 

*  Pour les petits "learning rates", l'entrainement du modèle va prendre plus de temps pour atteindre la valeur optimale des poids. 



1.   Premièrement on calcule le gradient ou la dérivée partielle de la "loss function" ("cost function") par rapport au poids (m et b)   
2.   Ensuite on choisit une valeures aléatoires pour les poids (m et b) et substitut 
3.   Aprés cela on calcule le "step size" (combien de fois on va mettre à jour les paramètres)
4.   Enfin, une fois que le "step size" est trouvé on calcule le nouveau poids, en soustrayant la valeur du "step size" à la valeur du poids précédant.

## 8 - Comprendre la théorie et l'intuition derrière les réseaux de neurones et le RESNETS![6.PNG](data:image/png;base64,

Comme on le voit dans le modèle de réseau de neurones ci-dessus, lorsque l'image rentre en input, on y applique ce que l'on appelle une convolution, c'est-à-dire qu'on va essayer d'extraire les features à partir de l'image d'input , ce qui est aussi appelé "Kernels ou Feature Detectors" et sont contenus dans le Layer Convolutionnel. Ensuite on va appliquer sur ces features ce que l'on appelle du "Pooling"(Regroupement) ou "DownSampling" (Echantillonage), ce processus s'effectue en recupérant les cartes de features puis les compresser ou réduire leurs tailles juste pour réduire l'exigence de la puissance de traitement lors de l'entrainement du modèle.
Une fois cela effectué, on peut maintenant prendre les pixels contenues dans "Pooling Layer" et y effectuer du "Flattening" (ou Aplatissement) des valeurs de pixels. 
Si on fini cette étape on prend les valeurs de pixels aplaties pour alimenter des réseaux neuronaux artificiels complexes. 

* Par exemple dans ce projet à réaliser on va construire 2 réseaux neuronaux artificiels, d'où le premier va servir à détecter les points faciales clés et l'autre va permettre de faire la détection d'émotion (colèreux, joyeux, triste, ...)

### **RESNET (Residual Network)**

* Plus les CNN s'agrandissent en profondeur, la disparition ou la descente du gradient tend à se produire ce qui impacte négativement la performance du réseau car le gradient sera tellement petit qu'il pourra plus mettre à jour les poids du réseau neuronal 

* Le problème de la descente du gradient se produit quand le gradient est rétro-propagé (back-propagated) aux layers (couches)  précédentes, ce qui se traduit par une très faible pente.

* Le "Residual Neural Network" inclu la fonctionnalité "skip connection" qui permet l'entrainement de 152 layers sans problème de disparition du gradient.

* Le RESNET marche en ajoutant du "identity mappings" en haut du CNN.

* "ImageNet" est une base de donnéees qui contient 11 millions d'images dont 11000 catégories.

* Cette base de donnée est utilisée pour entrainer le réseau neuronal profond "ResNet"




**Architecture du Modèle à constuire**





## 9 - Contruire un modèle de détection des points faciaux clés avec un réseau neuronal résiduel profond
"""

# La fonction pour construire le bloc résiduel (Res-Block)
def res_block(X, filter, stage):

  # Convolution Block
  X_copy = X

  f1, f2, f3 = filter

  # Main Path
  X = Conv2D(f1, (1,1),strides = (1,1), name = "res_"+str(stage)+"_conv_a", kernel_initializer = glorot_uniform(seed = 0))(X)
  X = MaxPool2D((2,2))(X)
  X = BatchNormalization(axis=3, name = "bn_"+str(stage)+"_conv_a")(X)
  X = Activation("relu")(X)

  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)
  X = Activation('relu')(X) 

  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)


  # Short path
  X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)
  X_copy = MaxPool2D((2,2))(X_copy)
  X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)

  # ADD
  X = Add()([X,X_copy])
  X = Activation('relu')(X)

  # Identity Block 1
  X_copy = X


  # Main Path
  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)
  X = Activation('relu')(X) 

  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)
  X = Activation('relu')(X) 

  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)

  # ADD
  X = Add()([X,X_copy])
  X = Activation('relu')(X)

  # Identity Block 2
  X_copy = X


  # Main Path
  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)
  X = Activation('relu')(X) 

  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)
  X = Activation('relu')(X) 

  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)

  # ADD
  X = Add()([X,X_copy])
  X = Activation('relu')(X)

  return X

# Le réseau de neurones convolutifs
input_shape = (96, 96, 1)

# Input tensor shape
X_input = Input(input_shape)

# Zero-padding
X = ZeroPadding2D((3,3))(X_input)

# 1 - stage
X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis =3, name = 'bn_conv1')(X)
X = Activation('relu')(X)
X = MaxPooling2D((3,3), strides= (2,2))(X)

# 2 - stage
X = res_block(X, filter= [64,64,256], stage= 2)

# 3 - stage
X = res_block(X, filter= [128,128,512], stage= 3)


# Average Pooling
X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)

# Final layer
X = Flatten()(X)
X = Dense(4096, activation = 'relu')(X)
X = Dropout(0.2)(X)
X = Dense(2048, activation = 'relu')(X)
X = Dropout(0.1)(X)
X = Dense(30, activation = 'relu')(X)


model_1_facialKeyPoints = Model( inputs= X_input, outputs = X)
model_1_facialKeyPoints.summary()

"""## 10 - Compiler et entrainer le modèle de Deep Learning pour la détection des points faciaux clés"""

adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)
model_1_facialKeyPoints.compile(loss = "mean_squared_error", optimizer = adam , metrics = ['accuracy'])
# Check this out for more information on Adam optimizer: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam

# save the best model with least validation loss
checkpointer = ModelCheckpoint(filepath = "FacialKeyPoints_weights.hdf5", verbose = 1, save_best_only = True)

history = model_1_facialKeyPoints.fit(X_train, y_train, batch_size = 32, epochs = 100, validation_split = 0.05, callbacks=[checkpointer])

# save the model architecture to json file for future use

model_json = model_1_facialKeyPoints.to_json()
with open("FacialKeyPoints-model.json","w") as json_file:
  json_file.write(model_json)

"""## 11 - Evaluer les performances du modèle de détection des points faciaux clés 

"""

with open('FacialKeyPoints-model.json', 'r') as json_file:
    json_savedModel= json_file.read()
    
# load the model architecture 
model_1_facialKeyPoints = tf.keras.models.model_from_json(json_savedModel)
model_1_facialKeyPoints.load_weights('FacialKeyPoints_weights.hdf5')
adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model_1_facialKeyPoints.compile(loss="mean_squared_error", optimizer= adam , metrics = ['accuracy'])

# Evaluate the model

result = model_1_facialKeyPoints.evaluate(X_test, y_test)
print("Accuracy : {}".format(result[1]))

# Get the model keys 
history.history.keys()

# Plot the training artifacts

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train_loss','val_loss'], loc = 'upper right')
plt.show()

"""# Partie 2 : DETECTION DES EXPRESSIONS FACIALES

## 12 - Importer et Explorer le dataset pour la détection d'expression faciale
"""

# read the csv files for the facial expression data
facialexpression_df = pd.read_csv(dataset_Dr+'icml_face_data.csv')

facialexpression_df

facialexpression_df[' pixels'][0] # String format

# function to convert pixel values in string format to array format

def string2array(x):
  return np.array(x.split(' ')).reshape(48, 48, 1).astype('float32')

# Resize images from (48, 48) to (96, 96)

def resize(x):
  
  img = x.reshape(48, 48)
  return cv2.resize(img, dsize=(96, 96), interpolation = cv2.INTER_CUBIC)

facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: string2array(x))

facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: resize(x))

facialexpression_df.head()

# check the shape of data_frame
facialexpression_df.shape

# check for the presence of null values in the data frame
facialexpression_df.isnull().sum()

label_to_text = {0:'anger', 1:'disgust', 2:'sad', 3:'happiness', 4: 'surprise'}

"""### **MINI CHALLENGE**

* Visualiser la première image dans le dataframe et vérifier que les images ne sont pas distortu par l'opération de redimensionnement ou de retaillement
"""

plt.imshow(facialexpression_df[' pixels'][0], cmap = 'gray')

"""## 13 - Visualiser les images et représenter les labels 

"""

emotions = [0, 1, 2, 3, 4]

for i in emotions:
  data = facialexpression_df[facialexpression_df['emotion'] == i][:1]
  img = data[' pixels'].item()
  img = img.reshape(96, 96)
  plt.figure()
  plt.title(label_to_text[i])
  plt.imshow(img, cmap = 'gray')

"""### **MINI CHALLENGE**

* Représenter un diagramme en barres pour ressortir combien d'images sont présentes par émotions. 
"""

facialexpression_df.emotion.value_counts().index

facialexpression_df.emotion.value_counts()

plt.figure(figsize = (10,10))
sns.barplot(x = facialexpression_df.emotion.value_counts().index, y = facialexpression_df.emotion.value_counts())

"""## 14 - Effectuer la préparation des données et l'augmentation d'images

*  Dans l'actvité précédente j'ai fait la représentation graphique du nobre d'images présentes par classes et on a vu que le dataset est non balancé donc ce qu'on doit faire c'est d'essayer de balancer le dataset en faisant de l'augmentation de données pour les classes dont le nombre d'images est faible.

"""

# Découper le dataset en "features" et "labels"
from keras.utils import to_categorical

X = facialexpression_df[" pixels"]
y = to_categorical(facialexpression_df["emotion"])

X[0]

X = np.stack(X, axis =0)
X = X.reshape(24568, 96, 96, 1)

X[0]

y

print(X.shape, y.shape)

# découper le dataframe en dataframe de train, de test et de validation
from sklearn.model_selection import train_test_split

X_train, X_test, y_Train, y_Test = train_test_split(X, y, test_size = 0.1, shuffle = True)
X_val, X_test, y_Val, y_Test = train_test_split(X_test, y_Test, test_size = 0.5, shuffle = True)

print(X_val.shape, y_Val.shape)

print(X_train.shape, y_Train.shape)

print(X_test.shape, y_Test.shape)

# Image pre-processing

X_train = X_train / 255
X_val = X_val / 255
X_test = X_test / 255

X_train

train_datagen = ImageDataGenerator(
    rotation_range = 15,
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    shear_range = 0.1,
    zoom_range = 0.1,
    horizontal_flip = True,
    #vertical_flip = True,
    #brightness_range = [1.1, 1.5], 
    fill_mode = "nearest"

)

"""## 15 - Construire et Entrainer un Modèle de Deep Learning pour la Classification des Expressions Faciales """

# Construction du Réseau de Neurones
input_shape = (96, 96, 1)

# Dimension du Tenseur d'Entrée
X_input = Input(input_shape)

# Zero-padding
X = ZeroPadding2D((3, 3))(X_input)

# 1 - Stage
X = Conv2D(64, (7, 7), strides= (2, 2), name = "conv1", kernel_initializer = glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis = 3, name = "bn_conv1")(X)
X = Activation("relu")(X)
X = MaxPooling2D((3, 3), strides = (2, 2))(X)

# 2 - Stage
X = res_block(X, filter = [64, 64, 256], stage = 2)

# 3 - Stage
X = res_block(X, filter = [128, 128, 512], stage = 3)

# 4 - Stage
#X = res_block(X, filter = [256, 256, 1024], stage = 4)

# Average Pooling
X = AveragePooling2D((4, 4), name = "Averagea_Pooling")(X)

# Final Layer
X = Flatten()(X)
X = Dense(5, activation = "softmax", name = "Dense_final", kernel_initializer = glorot_uniform(seed = 0))(X)

model_2_emotion = Model(inputs = X_input, outputs = X, name = "Resnet18")

model_2_emotion.summary()

# Entrainer le réseau
# Il faut noter que pour le "loss" on utilise le "categorical_crossentropy" car on plus de 2 catégories d'output
# Dans le cas où on a seulement 2 catégories on peut utiliser le "binary_crossentropy"
model_2_emotion.compile(optimizer = "Adam", loss = "categorical_crossentropy", metrics = ["accuracy"])

# On se rappellent que les premiers points faciaux clés du modèle sont enregistrés dans les fichiers suivants :
# ---> FacialKeyPoints_weights.hdf5 and FacialKeyPoints-model.json 

# En utilisant le "EarlyStopping" on quitte l'entrainement si le loss de la validation arrête de diminuer 
# même après un certains nombres d'epochs
earlystopping = EarlyStopping(monitor = "val_loss", mode = "min", verbose = 1, patience = 20)

# Enregistrer le meilleur modèle avec un loss minimal
checkpointer = ModelCheckpoint(filepath = "FacialExpression_weights.hdf5", verbose = 1, save_best_only = True)

"""**! Ici l'augmentation de données sera intégrée dans l'entrainement dont "train_datagen.flow()**"""

# Entrainement
history = model_2_emotion.fit(train_datagen.flow(X_train, y_Train, batch_size = 64),
                              validation_data = (X_val, y_Val), steps_per_epoch = len(X_train) // 64,
                              epochs = 50, callbacks = [checkpointer, earlystopping])

# Enregistrer l'architecture du modèle dans un fichier JSON pour utilisation future 
model_json = model_2_emotion.to_json()
with open("FacialExpression-model.json", "w") as json_file:
  json_file.write(model_json)

"""## 16 - Comprendre comment évaluer le modèle de classification 

* Matrice de Confusion (Confusion Matrix)
* Exactitude (ACCURACY)
* Précision (PRECISION)
* Retrait (RECALL)

**---> Matrice de Confusion (Confusion Matrix)**

**--->  Les KPIs (Les Indicateurs de Performances Clés)**

* Une Matrice de Confusion est utilisée pour décrire les performances du modèle de classification : 
   * True Positves (TP) : est le cas où le classifieur prédit "True" par exemple ils ont une maladie et que la vraie classe est "True" (le patient a une maladie).

   * True Negatives (TN) : le cas où le modèle prédit "False" (pas de maladie par exemple) et que la vraie classe est "False" (le patient n'a as de pas maladie)

   * False Positives (FP) (Type I error) : le classifieur prédit "True" mais la classe correcte est "False" (le patient n'a pas de maladie)

   * False Negatives (FN) (Type II error) : le classifieur prédit "False" mais la vraie classe est "True"

   * Classiication Accuracy = (TP + TN) / (TP + TN + FP + FN)

   * Misclassification rate (Error Rate) = (FP + FN) / (TP + TN + FP + FN)

   * Precision = TP / Total TRUE Predictions = TP / (TP + FP) (Lorsque le modèle prédit la classe "True", à quelle précision le modèle est-il juste ?

   * Recall = TP / Actuel TRUE = TP / (TP + FN) (orsque la vraie classe est "True" , à quelle degrée le modéle est-il précis ?


* L'Exactitude (Accuracy) est généralement trompeur et il n'est pas suffisant pour évaluer les performances du classifieur 

* Le "Recall" est un important indicateur de performance dans les situations où :

   * Le Dataset est non balancé (équilibré), par exemple le cas où on a petit nombre de patients ayant le cancer comparé à un gros nombre de patients en bonne santé

## 17 - Evaluer la performance du modèle de classification des expressions faciales
"""

with open("FacialExpression-model.json", "r") as json_file:
  json_savedModel = json_file.read()

# Charger l'architecture du modèle
model_2_emotion = tf.keras.models.model_from_json(json_savedModel)
model_2_emotion.load_weights("FacialExpression_weights.hdf5")
model_2_emotion.compile(optimizer = "Adam", loss = "categorical_crossentropy", metrics = ["accuracy"])

# Evaluation du modèle
 score =model_2_emotion.evaluate(X_test, y_Test)
 print("Test Accuracy : {}".format(score[1]))

# Les différents KPIs 
history.history.keys()

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(accuracy))
epochs

# Représentation Graphique de l'accuracy de l'entrainement et de la validation
plt.plot(epochs, accuracy, "bo", label = "Training Accuracy")
plt.plot(epochs, val_accuracy, "b", label = "Validation Accuracy")
plt.title("Training and Validation Accuracy")
plt.legend()

# Représentation Graphique du Loss de l'entrainement et de la validation
plt.plot(epochs, loss, "ro", label = "Training Loss")
plt.plot(epochs, val_loss, "r", label = "Validation Loss")
plt.title("Training and Validation Loss")
plt.legend()

# On récupère les classes prédites et les vraies classes
predicted_classes = np.argmax(model_2_emotion.predict(X_test), axis = -1)
y_true = np.argmax(y_Test, axis = -1)

y_true.shape

# Représentation de la Matrice de Confusion
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, predicted_classes)
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True, cbar = False)

# représentation de 25 images avec une prédiction de l'émotion présente sur l'image
L = 5
W = 5 

fig, axes = plt.subplots(L, W, figsize = (24, 24))
axes = axes.ravel()

for i in np.arange(0, L*W):
  axes[i].imshow(X_test[i].reshape(96, 96), cmap = "gray")
  axes[i].set_title("Prediction = {}\n True = {}".format(label_to_text[predicted_classes[i]], label_to_text[y_true[i]]))
  axes[i].axis("off")

plt.subplots_adjust(wspace = 1)

# Représentation des KPIs
from sklearn.metrics import classification_report
print(classification_report(y_true, predicted_classes))

"""# Partie 3 : Combiner les 2 modèles dont la détection dees points faciaux clés et la classification des expressions faciales

## 18 - Effectuer des Prédictions sur les 2 modèles
"""

def predict(X_test):
  #Effectuer des prédictions à partir du modèle des points faciaux clés
  df_predict = model_1_facialKeyPoints.predict(X_test)

  # Faire des predictions à partir de modèle des émotions
  df_emotion  = np.argmax(model_2_emotion.predict(X_test), axis = -1)

  # Redimensionner l'array de (856,) à (856, 1)
  df_emotion = np.expand_dims(df_emotion, axis = 1)

  # Convertir les prédictions en Dataframe
  df_predict = pd.DataFrame(df_predict, columns   = columns)

  # Ajouter les émotions sur un Dataframe prédit
  df_predict["emotion"] = df_emotion

  return df_predict

# Exécuter la fonction "predict"
df_predict = predict(X_test)

df_predict.head()

"""### **MINI CHALLENGE**
* Représenter une grille 16 images ainsi que leurs émotions et leurs points faciaux clés

"""

#Voyons plus d'images sur un format en grille
fig, axes  = plt.subplots(4, 4, figsize = (24, 24))
axes = axes.ravel()

for i in range(16):
  axes[i].imshow(X_test[i].squeeze(), cmap ="gray")
  axes[i].set_title("Prediction = {}".format(label_to_text[df_predict["emotion"][i]]))
  axes[i].axis("off")
  for j in range(1, 31, 2):
    axes[i].plot(df_predict.loc[i][j-1], df_predict.loc[i][j], "rx")

"""### **---> Déploiement du modèle en utilisant "Tensorflow Serving"**

* Supposons qu'on a déja entrainé notre modèle et il génére de bons résultats sur les données de test

* Maintenant on veut intégré notre modèle Tensorflow  entrainé dans une application web et déployer le modèle dans un environnement de niveau production.

* L'objectif précédant peut être atteint en utilisant le "Tensorflow Serving" système de service trés performant pour des modèles de Machine Learning, conçu pour des environnments de production

* Avec l'aide du "Tensorflow Serving", on peut facilement déployer de nouveaux algorithmes pour faire des prédictions

* Dans le but de servir le modèle entrainé en utilisant le "Tensorflow Serving", on a besoin d'enregistrer le modèle dans un format qui est favorable pour le service en utilisant "Tensorflow Serving"

* Le modèle va avoir un numéro de version et il sera enregistré sur une direction structurée.

* Après que le modèle soit sauvé, on peut maintenant utiliser le "Tensorflow Serving" pour commencer à faire une requête d'inférence en utilisant une version spécifique de notre "servable" modèle entrainé.


### **---> Exécuter le "Tensorflow Serving"**

* Ci-dessous certains paramétres importantes

  * rest_api_port : Le port qu'on va utiliser pour la requête REST
  * model_name : On va l'utiliser dans l'URL de la requête REST. On peut choisir n'importe quel nom
  * model_base_path : C'est le chemin vers la direction où j'ai enregistré notre modèle 

* Pour plus d'informations concernant le "REST", voire : [Code Academy - REST](https://www.codeacademy.com/articles/what-is-rest)

* Le "REST" est une renaissance du "HTTP" dans lequel les commandes HTTP ont une signification sémantique (significative)

### **---> Effectuer des requêtes dans le "Tensorflow Serving"**

* Dans le but de faire des prédictions en utilisant le "Tensorflow Serving, on a besoin de passer la requête inférencielle (Données Images) à un objet JSON

  * Inférence = Opération par laquelle on passe d'une assertion considérée comme vraie à une autre assertion au moyen d'un système de règles qui rend cette deuxième assertion également vraie.

* Ensuite, on utilise les librairies de requête Python pour faire une requête POST au modèle déployé en passant dans l'objet JSON la requête contenant l'inférence (données images)

* Finallemnt, on obtient les predictions à partir de la requête POST fait au modèle déployé et puis utiliser la fonction "argmax" pour trouver les classes prédites.

### **---> NOTE :**

* Maintenant on besoin d'enregistrer notre modèle entrainé et il doit être enregitré sous le format du "SavedModel" 

* Le modèle va avoir un numéro de version et il sera enregistré dans une direction structurée.

* **"tf.saved_model.save"** est une fonction utilisée pour construire le modèle sauvé qui est adéquat pour le service en utilisant "Tensorflow Serving"

* Aprés que le modèle soit sauvé, on peut maintenant utiliser le "Tensorflow Serving" afin de commencer à faire des requêtes inférencielles en utilisant une version spécifique de modèle "servable" entrainé.

* Utiliser le **"SavedModel"** pour enregistrer et charger les variables du modèle, le graphique (**graph**), les métadonnées du graphique.

* Voire pour plus d'informations :  [SavedModel](https://www.tensorflow.org/guide/saved_model)
"""

import json
import tensorflow.keras.backend as KeyboardInterrupt

def deploy(directory, model):
  MODEL_DIR = directory
  version = 1 

  # On joint la direction temporelle du modèle avec le numéro de la version choisie
  export_path = os.path.join(MODEL_DIR, str(version))
  print("export_path = {}\n".format(export_path))

  # On enregistre le modèle en utilisant "saved_model.save"
  # Si la direction existe déjà, on va la supprimer en utilisant la commande "!rm" 
  # Cette commande efface chaque fichier spécifié dans la ligne de commande
  if os.path.isdir(export_path):
    print("\nAlready saved a model, cleaning up\n")
    !rm -r {export_path}

  tf.saved_model.save(model, export_path)
  os.environ["MODEL_DIR"] = MODEL_DIR

"""## 20 - Servir le modèle en utilisant **Tensorflow Serving**"""

# On va ajouter le package "tensorflow-model-server" à notre liste de packages
!echo "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \
curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -
!apt update

# On installe le "tensorflow-model-server"
!apt-get install tensorflow-model-server

"""---> **On exécute le Tensorflow Serving**

* On va charger notre modèle et commencer à faire des inférences (prédictions) en se basant sur le modèle déja entrainé
"""

# On déploie le modèle de détection des points faciaux clés
deploy("/model", model_1_facialKeyPoints)

# Commented out IPython magic to ensure Python compatibility.
# # Lancement du serveur sur le Port: 4500
# %%bash --bg
# nohup tensorflow_model_server \
#   --rest_api_port=4500 \
#   --model_name=keypoint_model \
#   --model_base_path="${MODEL_DIR}" >server.log 2>&1

# Maintenant on voit ce qui se passe dans le background contenu dans fichier "log"
!tail server.log

# On déploie le modèle de détection des émotions
deploy("/model1", model_2_emotion)

# Commented out IPython magic to ensure Python compatibility.
# # Lancement du serveur sur le Port: 4000
# %%bash --bg
# nohup tensorflow_model_server \
#   --rest_api_port=4000 \
#   --model_name=emotion_model \
#   --model_base_path="${MODEL_DIR}" >server.log 2>&1

# Maintenant on voit ce qui se passe dans le background contenu dans fichier "log"
!tail server.log

"""* **Félicitations ! Maintenant on a chargé avec succés une version servable de notre modèle { *name* : keypoint_model *version* : 1}** 

* **Félicitations ! Maintenant on a chargé avec succés une version servable de notre modèle { *name* : emotion_model *version* : 1}**

## 21 - Effectuer des Requêtes sur le Modèle Servi sur **Tensorflow Serving**
"""

import json 

# On crée un objet JSON et faire 3 requêtes inférencielles

data = json.dumps({"signature_name": "serving_default", "instances": X_test[0:3].tolist()})
print("Data: {} ... {}".format(data[:50], data[len(data)-52:]))

X_test[0:3]

!pip install -q requests

import requests
# La fonction pour faire des prédictions à partir des modèles déployés 
def response(data):
  headers = {"content-type": "application/json"}
  json_response = requests.post("http://localhost:4500/v1/models/keypoint_model/versions/1:predict", data=data, headers=headers, verify=False)
  df_predict = json.loads(json_response.text)["predictions"]
  json_response = requests.post("http://localhost:4000/v1/models/emotion_model/versions/1:predict", data=data, headers=headers, verify=False)
  df_emotion = np.argmax(json.loads(json_response.text)["predictions"], axis=1)

  # Redimensionner l'array de (856,) à (856, 1)
  df_emotion = np.expand_dims(df_emotion, axis = 1)

  # Convertir les prédictions en Dataframe
  df_predict = pd.DataFrame(df_predict, columns=columns)

  # Ajouter les émotions dans le Dataframe des prédictions
  df_predict["emotion"] = df_emotion

  return df_predict

# Faire les prédictions sur de nouvelles données
df_predict = response(data)
df_predict

# Représenter Graphiquement les images de tests et leurs points clés prédits ainsi que leurs émotions

fig, axes = plt.subplots(3, 1, figsize = (24, 24))
axes = axes.ravel()

for i in range(3):
  axes[i].imshow(X_test[i].squeeze(), cmap = "gray")
  axes[i].set_title("Prediction = {}".format(label_to_text[df_predict["emotion"][i]]))
  axes[i].axis("off")
  for j in range(1, 31, 2):
    axes[i].plot(df_predict.loc[i][j-1], df_predict.loc[i][j], "rx")

"""#################################################
########### EXCELLENT TRAVAIL :) XD ! ###########
#################################################
"""

